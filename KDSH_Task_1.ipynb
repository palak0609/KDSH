{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries and Installation\n",
    "This project utilizes several key Python libraries. To ensure proper execution, please install them using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exact Pip Command for Installation:\n",
    "\n",
    "# pip install spacy sentence-transformers scikit-learn pandas numpy PyPDF2 textstat seaborn matplotlib\n",
    "##Installs core libraries for NLP, ML, data handling, and PDF processing.\n",
    "\n",
    "##Downloads the small English spaCy language model for text analysis.\n",
    "# python (or python3) -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialization\n",
    "This section imports necessary libraries and initializes key components for our paper classification pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/specter. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textstat import flesch_reading_ease \n",
    "\n",
    "\n",
    "# Initialize NLP components and models\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "embedding_model = SentenceTransformer('allenai/specter')\n",
    "scaler = StandardScaler()\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading PDF\n",
    "#### <small>This Python function <b>read_pdf</b> extracts text content from a PDF file using the PdfReader library. Its job in a binary classifier context is to convert the raw PDF documents (input features) into text strings, which can then be processed (e.g., tokenized, vectorized) and used as input for the classification model (e.g., to predict whether a paper is publishable or not). </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "#### <small>This <b>preprocess_text</b> function cleans and prepares the extracted text from PDFs for use in a machine learning model. It removes extra whitespace and citation numbers, then uses spaCy for tokenization, filtering out stop words and punctuation to retain only meaningful words for the binary classification task.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'\\[[\\d,\\s]+\\]', '', text)  # Remove citation numbers\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Keep only meaningful tokens\n",
    "    tokens = [token.text for token in doc \n",
    "              if not token.is_stop and not token.is_punct \n",
    "              and token.text.strip()]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "The `extract_features` function processes the preprocessed text of each paper to create a set of numerical features. These features armadeed to capture various aspects of the paper's content and structure, which can be useful for distinguishing between different types of papers or predicting publishability. The extracted features include:\n",
    "\n",
    "*   **Structural Features:** Presence of key sections like abstract, introduction, methodology, results, and conclusion.\n",
    "*   **Content Quality Features:** Count of citations, equations, figures, and tables, which can indicate the depth and rigor of the research.\n",
    "*   **Readability and Complexity Features:** Flesch reading ease score, word count, and average word length, providing insights into the writing style and complexity of the paper.\n",
    "*   **Technical Content Density:** Ratio of technical keywords (e.g., \"algorithm,\" \"method,\" \"analysis\") to the total word count, reflecting the technical focus of the paper.\n",
    "\n",
    "These features are then used as input for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "     This function extracts numerical features from the paper text that might \n",
    "     be informative for classifying research papers. \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Basic structure features (presence of common section headings)\n",
    "    features['has_abstract'] = int(bool(re.search(r'\\b(abstract)\\b', text.lower())))\n",
    "    features['has_introduction'] = int(bool(re.search(r'\\b(introduction|background)\\b', text.lower())))\n",
    "    features['has_methodology'] = int(bool(re.search(r'\\b(methodology|methods|approach)\\b', text.lower())))\n",
    "    features['has_results'] = int(bool(re.search(r'\\b(results|findings)\\b', text.lower())))\n",
    "    features['has_conclusion'] = int(bool(re.search(r'\\b(conclusion|conclusions|summary)\\b', text.lower())))\n",
    "    \n",
    "    # Content quality features\n",
    "    features['citation_count'] = len(re.findall(r'\\[\\d+\\]|\\(\\w+\\s*,\\s*\\d{4}\\)', text))  # Count citations in various formats\n",
    "    features['equation_count'] = len(re.findall(r'\\$.*?\\$', text)) # Count occurrences of LaTeX expressions\n",
    "    features['figure_count'] = len(re.findall(r'\\b(figure|fig\\.)\\s*\\d+\\b', text.lower())) # Count mentions of figures by number\n",
    "    features['table_count'] = len(re.findall(r'\\b(table|tbl\\.)\\s*\\d+\\b', text.lower())) # Count mentions of tables by number\n",
    "    \n",
    "    # Readability and complexity\n",
    "    features['reading_score'] = flesch_reading_ease(text)  # Assuming flesch_reading_ease function is imported elsewhere\n",
    "    features['word_count'] = len(text.split())   # Count total words\n",
    "    features['average_word_length'] = np.mean([len(word) for word in text.split()]) # Calculate average word length\n",
    "    \n",
    "    # Technical content density\n",
    "    features['technical_word_ratio'] = len(re.findall(r'\\b(algorithm|method|implementation|analysis|evaluation)\\b', text.lower())) / max(1, features['word_count'])\n",
    "    # Count technical terms divided by word count (to avoid division by zero)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Embeddings\n",
    "\n",
    "The `generate_embedding` function creates a document embedding (vector representation) of the input text using the SPECTER model. This embedding captures the semantic meaning of the text and is used as input for the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text):\n",
    "    \"\"\"Generate document embedding using SPECTER.\"\"\"\n",
    "    return embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Features\n",
    "\n",
    "The `prepare_features` function combines the extracted numerical features and the document embedding into a single feature vector. This combined vector serves as the final input to the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(features_dict, embedding):\n",
    "    \"\"\"Combine numerical features and embedding into single feature vector.\"\"\"\n",
    "    numerical_features = np.array(list(features_dict.values()))\n",
    "    return np.concatenate([numerical_features, embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The `prepare_data` function takes a list of labeled paper paths and performs the full data preprocessing pipeline: reading the PDF, preprocessing the text, extracting features (numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(labeled_papers):\n",
    "    \"\"\"Prepare features and labels from labeled papers.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for paper_path, label in labeled_papers:\n",
    "        # Read and process paper\n",
    "        text = read_pdf(paper_path)\n",
    "        if text is None:\n",
    "            continue\n",
    "        \n",
    "        processed_text = preprocess_text(text)\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(text)\n",
    "        embedding = generate_embedding(processed_text)\n",
    "        \n",
    "        # Combine features\n",
    "        X.append(prepare_features(features, embedding))\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "This section details the training and evaluation of our classification model. The `train_classifier` function orchestrates the process:\n",
    "\n",
    "1.  **Data Preparation:** The `prepare_data` function (not shown) extracts features (X) and labels (y) from the labeled papers.\n",
    "2.  **Train-Test Split:** Data is split into 80% training and 20% testing sets using `train_test_split` with stratification to maintain class balance.\n",
    "3.  **Feature Scaling:** Features are scaled using a pre-defined `scaler` (e.g., `StandardScaler`) to improve model performance.\n",
    "4.  **Model Training:** A pre-defined `classifier` is trained on the scaled training data.\n",
    "5.  **Prediction and Evaluation:** Predictions are made on the test set, and performance is evaluated using the F1 score and a detailed classification report. The F1 score is returned for model comparison.\n",
    "\n",
    "This robust training pipeline ensures reliable model evaluation and facilitates comparison between different classifier choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(labeled_papers):\n",
    "    \"\"\"Train the classifier using labeled papers and visualize results.\"\"\"\n",
    "    # Prepare features and labels\n",
    "    X, y = prepare_data(labeled_papers)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # y_proba = classifier.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Publishability Prediction\n",
    "\n",
    "The `predict_paper` function predicts a paper's publishability given its file path\n",
    "\n",
    "1.  **Text Extraction & Preprocessing:** Extracts text from the PDF using `read_pdf` and preprocesses it with `preprocess_text`. Returns `None` if PDF reading fails.\n",
    "2.  **Feature Engineering:** Extracts features (`extract_features`) from the raw text and generates a text embedding (`generate_embedding`) from the preprocessed text.\n",
    "3.  **Feature Preparation & Scaling:** Combines features and embedding using `prepare_features`, then scales the resulting feature vector using the pre-trained `scaler`. Reshaping to (1, -1) ensures compatibility with the scaler.\n",
    "4.  **Prediction:** Uses the pre-trained `classifier` to predict publishability.\n",
    "5.  **Output:** Returns a dictionary containing the predction.\n",
    "\n",
    "This function efficiently predicts publishability using a trained model and pre-engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_paper(paper_path):\n",
    "    \"\"\"Predict if a paper is publishable.\"\"\"\n",
    "    # Read and process paper\n",
    "    text = read_pdf(paper_path)\n",
    "    if text is None:\n",
    "        return None\n",
    "    \n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(text)\n",
    "    embedding = generate_embedding(processed_text)\n",
    "    \n",
    "    # Prepare features\n",
    "    X = prepare_features(features, embedding)\n",
    "    X = scaler.transform(X.reshape(1, -1))\n",
    "    \n",
    "    # Make prediction\n",
    "    probability = classifier.predict_proba(X)[0]\n",
    "    prediction = classifier.predict(X)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        # 'confidence': probability[1] if prediction == 1 else probability[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution and Result Generation\n",
    "\n",
    "The `main` function orchestrates the entire paper publishability prediction process\n",
    "\n",
    "1.  **Labeled Data:** Defines a list of labeled papers (filename, label) for model training.\n",
    "2.  **Model Training:** Trains the classifier using `train_classifier` and prints the overall F1 score.\n",
    "3.  **Batch Prediction:** Iterates through all PDF files in the 'Papers' directory, predicting publishability for each using `predict_paper`. Handles potential PDF reading errors.\n",
    "4.  **Result Aggregation:** Stores the predictions (filename and publishability) in a list.\n",
    "5.  **Result Saving:** Saves the aggregated results to a 'results.csv' file using a Pandas Dat<small>Frame.\n",
    "\n",
    "This function provides a complete workflow for training the model and applying it to a batch of papers, saving the results for further</small> analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Example of labeled papers (path, label)\n",
    "    labeled_papers = [\n",
    "        ('R001.pdf', 0),\n",
    "        ('R002.pdf', 0),\n",
    "        ('R003.pdf', 0),\n",
    "        ('R004.pdf', 0),            \n",
    "        ('R005.pdf', 0),\n",
    "        ('R006.pdf', 1),\n",
    "        ('R007.pdf', 1),\n",
    "        ('R008.pdf', 1),\n",
    "        ('R009.pdf', 1),\n",
    "        ('R010.pdf', 1),\n",
    "        ('R011.pdf', 1),\n",
    "        ('R012.pdf', 1),\n",
    "        ('R013.pdf', 1),\n",
    "        ('R014.pdf', 1),\n",
    "        ('R015.pdf', 1)\n",
    "    ]\n",
    "    \n",
    "    # Train the classifier and get F1 score\n",
    "    f1 = train_classifier(labeled_papers)\n",
    "    print(f\"\\nOverall F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    # Directory containing papers to classify\n",
    "    papers_dir = 'Papers'\n",
    "    \n",
    "    # Classify all papers in directory\n",
    "    results = []\n",
    "    for filename in os.listdir(papers_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            paper_path = os.path.join(papers_dir, filename)\n",
    "            result = predict_paper(paper_path)\n",
    "            if result is not None:\n",
    "                results.append({\n",
    "                    'paper_id': filename,\n",
    "                    'publishable': result['prediction'],\n",
    "                    # 'confidence': result['confidence']\n",
    "                })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('results.csv', index=False)\n",
    "    print(\"\\nClassification complete. Results saved to results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Entry Point\n",
    "\n",
    "This section defines the entry point for the program. The `if __name__ == \"__main__\":` block is a standard Python construct that ensures the `main()` function, which orchestrates the entire paper classification workflow, is executed only when the script is run directls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "F1 Score: 0.800\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "Overall F1 Score: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification complete. Results saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
