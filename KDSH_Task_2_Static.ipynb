{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries and Installation\n",
    "This project utilizes several key Python libraries. To ensure proper execution, please install them using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exact Pip Command for Installation:\n",
    "\n",
    "# pip install spacy sentence-transformers scikit-learn pandas numpy PyPDF2 textstat seaborn matplotlib langchain\n",
    "##Installs core libraries for NLP, ML, data handling, and PDF processing.\n",
    "\n",
    "##Downloads the small English spaCy language model for text analysis.\n",
    "# python (or python3) -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialization\n",
    "This section imports necessary libraries and initializes key components for our paper classification pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "This function loads the pre-trained SPECTER model and its tokenizer, which are used for generating paper embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Initialize and load the SPECTER model and tokenizer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    model = AutoModel.from_pretrained('allenai/specter')\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Labeled Papers\n",
    "This list contains reference papers and their corresponding conferences, which will be used for training and creating conference-specific embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_papers = [\n",
    "    ('R006.pdf', 'CVPR'),\n",
    "    ('R007.pdf', 'CVPR'),\n",
    "    ('R008.pdf', 'EMNLP'),\n",
    "    ('R009.pdf', 'EMNLP'),\n",
    "    ('R010.pdf', 'KDD'),\n",
    "    ('R011.pdf', 'KDD'),\n",
    "    ('R012.pdf', 'NeurIPS'),\n",
    "    ('R013.pdf', 'NeurIPS'),\n",
    "    ('R014.pdf', 'TMLR'),\n",
    "    ('R015.pdf', 'TMLR')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Papers\n",
    "This function takes the text of a paper and generates an embedding using the SPECTER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, tokenizer, model):\n",
    "    \"\"\"Generate embedding for given text using SPECTER\"\"\"\n",
    "    max_length = 512\n",
    "    text = ' '.join(text.split()[:max_length])\n",
    "    \n",
    "    inputs = tokenizer(text, padding=True, truncation=True, \n",
    "                      max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from Papers\n",
    "This function extracts various features from the paperâ€™s content by counting the occurrences of specific keywords related to different research areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_features(text):\n",
    "    \"\"\"Extract key features from paper text using keyword matching\"\"\"\n",
    "    features = {\n",
    "        'deep_learning': len(re.findall(r'\\b(deep learning|neural network|CNN|RNN|LSTM|deep neural|artificial neural|convolutional|recurrent neural)\\b', text.lower())),\n",
    "        'computer_vision': len(re.findall(r'\\b(computer vision|image processing|object detection|segmentation|visual recognition|image classification|feature detection|opencv)\\b', text.lower())),\n",
    "        'nlp': len(re.findall(r'\\b(natural language|nlp|text mining|language model|transformer|bert|gpt|word embedding|tokenization|semantic analysis)\\b', text.lower())),\n",
    "        'data_mining': len(re.findall(r'\\b(data mining|clustering|pattern recognition|kdd|knowledge discovery|association rules|anomaly detection|predictive analytics)\\b', text.lower())),\n",
    "        'theory': len(re.findall(r'\\b(theorem|proof|lemma|theoretical|convergence|mathematical model|algorithm complexity|optimization theory)\\b', text.lower())),\n",
    "        'methodology': len(re.findall(r'\\b(methodology|experimental design|ablation study|comparative analysis|empirical study|statistical analysis)\\b', text.lower())),\n",
    "        'results': len(re.findall(r'\\b(results|performance|accuracy|precision|recall|f1 score|benchmark|evaluation metrics)\\b', text.lower()))\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conference Embeddings\n",
    "This function reads labeled reference papers, extracts content, generates embeddings, and stores features for each conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conference_embeddings(labeled_papers, tokenizer, model):\n",
    "    \"\"\"Create embeddings for each conference's reference papers\"\"\"\n",
    "    conference_papers = {}\n",
    "    conference_features = {}\n",
    "    \n",
    "    base_path = os.getcwd()  # Get current working directory\n",
    "    \n",
    "    for filename, conference in labeled_papers:\n",
    "        if conference not in conference_papers:\n",
    "            conference_papers[conference] = []\n",
    "            conference_features[conference] = []\n",
    "        \n",
    "        try:\n",
    "            paper_path = os.path.join(base_path, filename)\n",
    "            if not os.path.exists(paper_path):\n",
    "                print(f\"Reference paper not found: {paper_path}\")\n",
    "                continue\n",
    "                \n",
    "            loader = PyPDFLoader(paper_path)\n",
    "            pages = loader.load()\n",
    "            content = ' '.join([page.page_content for page in pages[:3]])\n",
    "            \n",
    "            conference_papers[conference].append(content)\n",
    "            conference_features[conference].append(extract_paper_features(content))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading reference paper {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    conference_embeddings = {}\n",
    "    for conference, papers in conference_papers.items():\n",
    "        paper_embeddings = [get_embedding(paper, tokenizer, model) for paper in papers]\n",
    "        conference_embeddings[conference] = paper_embeddings\n",
    "    \n",
    "    return conference_embeddings, conference_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend a Conference\n",
    "This function takes a new paper, compares it with reference papers, and recommends the most suitable conference based on similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_conference(new_paper_content, conference_embeddings, conference_features, tokenizer, model):\n",
    "    \"\"\"Recommend conference based on content similarity and features\"\"\"\n",
    "    new_paper_embedding = get_embedding(new_paper_content, tokenizer, model)\n",
    "    new_paper_features = extract_paper_features(new_paper_content)\n",
    "    \n",
    "    conference_scores = {}\n",
    "    feature_similarities = {}\n",
    "    \n",
    "    for conference, paper_embeddings in conference_embeddings.items():\n",
    "        # Calculate embedding similarities\n",
    "        similarities = []\n",
    "        for paper_embedding in paper_embeddings:\n",
    "            similarity = cosine_similarity([new_paper_embedding], [paper_embedding])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        top_similarities = sorted(similarities, reverse=True)[:2]\n",
    "        conference_scores[conference] = np.mean(top_similarities)\n",
    "        \n",
    "        # Calculate feature similarity\n",
    "        conf_features = conference_features[conference]\n",
    "        feature_matches = []\n",
    "        for paper_feat in conf_features:\n",
    "            total_features = sum(paper_feat.values()) + sum(new_paper_features.values())\n",
    "            if total_features == 0:\n",
    "                match_score = 0\n",
    "            else:\n",
    "                common_features = sum(min(paper_feat[k], new_paper_features[k]) for k in paper_feat)\n",
    "                match_score = 2 * common_features / total_features\n",
    "            feature_matches.append(match_score)\n",
    "        feature_similarities[conference] = np.mean(feature_matches)\n",
    "    \n",
    "    # Combine scores (70% embedding similarity, 30% feature similarity)\n",
    "    final_scores = {\n",
    "        conf: 0.7 * emb_score + 0.3 * feature_similarities[conf]\n",
    "        for conf, emb_score in conference_scores.items()\n",
    "    }\n",
    "    \n",
    "    return max(final_scores.items(), key=lambda x: x[1]), final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation Function\n",
    "\n",
    "This function generates detailed justifications for conference recommendations based on paper features and similarity scores. It analyzes various aspects like computer vision, NLP, data mining, and theoretical content to provide a comprehensive explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_justification(paper_features, conference, similarity_score):\n",
    "    \"\"\"Generate a detailed, specific justification for the conference classification\"\"\"\n",
    "    # Get feature counts and methodological aspects\n",
    "    total_features = sum(paper_features.values())\n",
    "    feature_strengths = sorted(\n",
    "        [(k, v) for k, v in paper_features.items() if v > 0],\n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Calculate percentages for top features\n",
    "    feature_percentages = {k: (v/total_features)*100 for k, v in feature_strengths[:3]} if total_features > 0 else {}\n",
    "    \n",
    "    # Build detailed justification based on conference and actual content\n",
    "    if conference == 'CVPR':\n",
    "        cv_focus = paper_features['computer_vision']\n",
    "        dl_focus = paper_features['deep_learning']\n",
    "        justification = (\n",
    "            f\"Paper demonstrates {cv_focus} computer vision concepts and {dl_focus} deep learning applications. \"\n",
    "            f\"Content analysis shows {feature_percentages.get('computer_vision', 0):.1f}% computer vision focus. \"\n",
    "            f\"Methodology includes {paper_features['methodology']} experimental components. \"\n",
    "            f\"Similarity score with CVPR papers: {similarity_score:.2f}. \"\n",
    "            f\"Strong alignment with computer vision research scope.\"\n",
    "        )\n",
    "\n",
    "    elif conference == 'EMNLP':\n",
    "        nlp_focus = paper_features['nlp']\n",
    "        method_focus = paper_features['methodology']\n",
    "        justification = (\n",
    "            f\"Contains {nlp_focus} natural language processing elements and {method_focus} methodological components. \"\n",
    "            f\"NLP content comprises {feature_percentages.get('nlp', 0):.1f}% of technical content. \"\n",
    "            f\"Includes {paper_features['results']} results-related discussions. \"\n",
    "            f\"Shows {similarity_score:.2f} similarity with EMNLP papers. \"\n",
    "            f\"Well-aligned with computational linguistics scope.\"\n",
    "        )\n",
    "\n",
    "    elif conference == 'KDD':\n",
    "        dm_focus = paper_features['data_mining']\n",
    "        results_focus = paper_features['results']\n",
    "        justification = (\n",
    "            f\"Exhibits {dm_focus} data mining concepts and {results_focus} experimental results. \"\n",
    "            f\"Data mining comprises {feature_percentages.get('data_mining', 0):.1f}% of content. \"\n",
    "            f\"Includes {paper_features['methodology']} methodology discussions. \"\n",
    "            f\"Similarity score with KDD papers: {similarity_score:.2f}. \"\n",
    "            f\"Strong focus on knowledge discovery and data mining.\"\n",
    "        )\n",
    "\n",
    "    elif conference == 'NeurIPS':\n",
    "        theory_focus = paper_features['theory']\n",
    "        dl_focus = paper_features['deep_learning']\n",
    "        justification = (\n",
    "            f\"Contains {theory_focus} theoretical concepts and {dl_focus} deep learning elements. \"\n",
    "            f\"Theoretical content makes up {feature_percentages.get('theory', 0):.1f}% of the paper. \"\n",
    "            f\"Includes {paper_features['methodology']} methodology components. \"\n",
    "            f\"Shows {similarity_score:.2f} similarity with NeurIPS papers. \"\n",
    "            f\"Strong theoretical machine learning focus.\"\n",
    "        )\n",
    "\n",
    "    elif conference == 'TMLR':\n",
    "        dl_focus = paper_features['deep_learning']\n",
    "        method_focus = paper_features['methodology']\n",
    "        justification = (\n",
    "            f\"Demonstrates {dl_focus} machine learning concepts and {method_focus} methodological elements. \"\n",
    "            f\"Technical content comprises {feature_percentages.get('deep_learning', 0):.1f}% ML focus. \"\n",
    "            f\"Contains {paper_features['results']} results-related discussions. \"\n",
    "            f\"Similarity score with TMLR papers: {similarity_score:.2f}. \"\n",
    "            f\"Well-suited for machine learning research scope.\"\n",
    "        )\n",
    "\n",
    "    # Ensure justification is between 50-100 words\n",
    "    words = justification.split()\n",
    "    if len(words) > 100:\n",
    "        justification = ' '.join(words[:100])\n",
    "    elif len(words) < 50:\n",
    "        justification += f\" The paper's technical depth and methodology align well with {conference}'s standards.\"\n",
    "    \n",
    "    return justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "This function orchestrates the entire paper processing pipeline:\n",
    "1. Loads the necessary models\n",
    "2. Creates conference embeddings\n",
    "3. Processes each paper\n",
    "4. Generates recommendations and justifications\n",
    "5. Saves results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers():\n",
    "    \"\"\"Main function to process all papers\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer, model = load_model()\n",
    "    \n",
    "    print(\"Creating conference embeddings...\")\n",
    "    conference_embeddings, conference_features = create_conference_embeddings(\n",
    "        labeled_papers, tokenizer, model\n",
    "    )\n",
    "    \n",
    "    print(\"Reading results.csv...\")\n",
    "    df = pd.read_csv('results.csv')\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['conference'] = 'NA'\n",
    "    df['justification'] = ''\n",
    "    \n",
    "    # Create the base path for the Papers folder\n",
    "    base_path = os.path.join(os.getcwd(), \"Papers\")\n",
    "    \n",
    "    print(\"Processing papers...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Processing paper {idx+1}/{len(df)}: {row['paper_id']}\")\n",
    "        \n",
    "        if row['publishable'] == 1:\n",
    "            try:\n",
    "                # Construct the full path to the PDF\n",
    "                paper_path = os.path.join(base_path, row['paper_id'])\n",
    "                \n",
    "                # Verify file exists before processing\n",
    "                if not os.path.exists(paper_path):\n",
    "                    print(f\"File not found: {paper_path}\")\n",
    "                    continue\n",
    "                \n",
    "                loader = PyPDFLoader(paper_path)\n",
    "                pages = loader.load()\n",
    "                paper_content = ' '.join([page.page_content for page in pages[:3]])\n",
    "                \n",
    "                (recommended_conference, score), _ = recommend_conference(\n",
    "                    paper_content, \n",
    "                    conference_embeddings, \n",
    "                    conference_features, \n",
    "                    tokenizer, \n",
    "                    model\n",
    "                )\n",
    "                \n",
    "                paper_features = extract_paper_features(paper_content)\n",
    "                \n",
    "                df.at[idx, 'conference'] = recommended_conference\n",
    "                df.at[idx, 'justification'] = generate_justification(\n",
    "                    paper_features, \n",
    "                    recommended_conference, \n",
    "                    score\n",
    "                )\n",
    "                \n",
    "                print(f\"Successfully classified as: {recommended_conference}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row['paper_id']}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"Saving results...\")\n",
    "    df.to_csv('final_results.csv', index=False)\n",
    "    print(\"Processing complete. Results saved to final_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Block\n",
    "\n",
    "Entry point of the script that initiates the paper processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Creating conference embeddings...\n",
      "Reading results.csv...\n",
      "Processing papers...\n",
      "Processing paper 1/135: P005.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 2/135: P011.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 3/135: P039.pdf\n",
      "Processing paper 4/135: P038.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 5/135: P010.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 6/135: P004.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 7/135: P012.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 8/135: P006.pdf\n",
      "Processing paper 9/135: P007.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 10/135: P013.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 11/135: P017.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 12/135: P003.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 13/135: P002.pdf\n",
      "Processing paper 14/135: P016.pdf\n",
      "Processing paper 15/135: P028.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 16/135: P014.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 17/135: P015.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 18/135: P001.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 19/135: P029.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 20/135: P099.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 21/135: P066.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 22/135: P072.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 23/135: P112.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 24/135: P106.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 25/135: P107.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 26/135: P113.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 27/135: P073.pdf\n",
      "Processing paper 28/135: P067.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 29/135: P098.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 30/135: P071.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 31/135: P065.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 32/135: P059.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 33/135: P105.pdf\n",
      "Processing paper 34/135: P111.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 35/135: P110.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 36/135: P104.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 37/135: P058.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 38/135: P064.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 39/135: P070.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 40/135: P048.pdf\n",
      "Processing paper 41/135: P074.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 42/135: P060.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 43/135: P128.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 44/135: P100.pdf\n",
      "Processing paper 45/135: P114.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 46/135: P115.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 47/135: P101.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 48/135: P129.pdf\n",
      "Processing paper 49/135: P061.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 50/135: P075.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 51/135: P049.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 52/135: P088.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 53/135: P063.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 54/135: P077.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 55/135: P117.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 56/135: P103.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 57/135: P102.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 58/135: P116.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 59/135: P076.pdf\n",
      "Processing paper 60/135: P062.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 61/135: P089.pdf\n",
      "Successfully classified as: TMLR\n",
      "Processing paper 62/135: P084.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 63/135: P090.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 64/135: P047.pdf\n",
      "Processing paper 65/135: P053.pdf\n",
      "Processing paper 66/135: P133.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 67/135: P127.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 68/135: P126.pdf\n",
      "Successfully classified as: TMLR\n",
      "Processing paper 69/135: P132.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 70/135: P052.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 71/135: P046.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 72/135: P091.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 73/135: P085.pdf\n",
      "Successfully classified as: TMLR\n",
      "Processing paper 74/135: P093.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 75/135: P087.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 76/135: P050.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 77/135: P044.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 78/135: P078.pdf\n",
      "Processing paper 79/135: P124.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 80/135: P130.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 81/135: P118.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 82/135: P119.pdf\n",
      "Processing paper 83/135: P131.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 84/135: P125.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 85/135: P079.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 86/135: P045.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 87/135: P051.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 88/135: P086.pdf\n",
      "Processing paper 89/135: P092.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 90/135: P096.pdf\n",
      "Processing paper 91/135: P082.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 92/135: P069.pdf\n",
      "Processing paper 93/135: P055.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 94/135: P041.pdf\n",
      "Processing paper 95/135: P109.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 96/135: P121.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 97/135: P135.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 98/135: P134.pdf\n",
      "Processing paper 99/135: P120.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 100/135: P108.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 101/135: P040.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 102/135: P054.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 103/135: P068.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 104/135: P083.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 105/135: P097.pdf\n",
      "Processing paper 106/135: P081.pdf\n",
      "Processing paper 107/135: P095.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 108/135: P042.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 109/135: P056.pdf\n",
      "Processing paper 110/135: P122.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 111/135: P123.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 112/135: P057.pdf\n",
      "Processing paper 113/135: P043.pdf\n",
      "Processing paper 114/135: P094.pdf\n",
      "Processing paper 115/135: P080.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 116/135: P024.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 117/135: P030.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Processing paper 118/135: P018.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 119/135: P019.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 120/135: P031.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 121/135: P025.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 122/135: P033.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 123/135: P027.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 124/135: P026.pdf\n",
      "Processing paper 125/135: P032.pdf\n",
      "Processing paper 126/135: P036.pdf\n",
      "Processing paper 127/135: P022.pdf\n",
      "Processing paper 128/135: P023.pdf\n",
      "Successfully classified as: CVPR\n",
      "Processing paper 129/135: P037.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 130/135: P009.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 131/135: P021.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 132/135: P035.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 133/135: P034.pdf\n",
      "Successfully classified as: EMNLP\n",
      "Processing paper 134/135: P020.pdf\n",
      "Successfully classified as: KDD\n",
      "Processing paper 135/135: P008.pdf\n",
      "Successfully classified as: NeurIPS\n",
      "Saving results...\n",
      "Processing complete. Results saved to final_results.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_papers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
